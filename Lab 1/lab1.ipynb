{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:40px;\"><center>Exercise I:<br> Training of simple MLP models\n",
    "</center></h1>\n",
    "\n",
    "# Introduction\n",
    "## Short summary\n",
    "In this exercise you will: \n",
    "\n",
    "* train MLPs for simple classification and regression problems.\n",
    "* learn how hyper-parameters such as learning rate, batch size and number of epochs affect training.\n",
    "\n",
    "You should write the report of the exercise within this notebook. The details of how to do that can be found below in section \"Writing the report\".\n",
    "\n",
    "**Deadline for submitting the report: See Canvas assignment.**\n",
    "\n",
    "## The data\n",
    "We will use two synthetic different data sets in this exercise\n",
    "\n",
    "### syn2\n",
    "The *syn2* dataset represents a binary classification problem. The input data is 2D which allows for an easy visual inspection of the different classes and the decision boundary implemented by the network. The dataset is generated using random numbers each time you run the cell. This means that each time you generate the data it will be slightly different. You can control this by having a fixed *seed* to the random number generator. The cell \"PlotData\" will plot the *syn2* dataset.\n",
    "\n",
    "### regr1\n",
    "The *regr1* dataset represents a regression problem. It has one input and one target variable. It a cosine function, with the possibility to add some noise and dampening on the output. Again see the cell \"PlotData\" to look at the dataset.\n",
    "\n",
    "## The exercises\n",
    "There are 8 questions in this exercise. These 8 questions can be found in three different cells below (see section \"The Different Cells\"). The first 6 questions will use the *regr1* dataset and questions 7-8 will use *syn2*.\n",
    "\n",
    "## The different 'Cells'\n",
    "This notebook contains several cells with python code, together with the markdown cells (like this one) with only text. Each of the cells with python code has a \"header\" markdown cell with information about the code. The table below provides a short overview of the code cells. \n",
    "\n",
    "| #  |  CellName | CellType | Comment |\n",
    "| :--- | :-------- | :-------- | :------- |\n",
    "| 2  | Init | Needed | Sets up the environment|\n",
    "| 3  | Data | Needed | Defines the functions to generate the artificial datasets |\n",
    "| 4  | PlotData | Information | Plots the 2D classification datasets |\n",
    "| 5  | MLP | Needed | Defines the MLP model |\n",
    "| 6  | Training | Needed | Functions for training and testing the MLP model |\n",
    "| 7  | Boundary | Needed | Functions for showing classification boundaries and errors | \n",
    "| 8  | Ex1 | Exercise | For question 1-4 |\n",
    "| 9  | Ex2 | Exercise | For question 5-6 |\n",
    "| 10 | Ex3 | Exercise | For question 7-8 |\n",
    "\n",
    "To start with the exercise you need to run all cells with the celltype \"Needed\". The very first time we suggest that you enter each of the needed cells, read the cell instruction and run the cell. It is important that you do this in the correct order, starting from the top and work your way down the cells. Later, when you have started to work with the notebook it may be easier to use the command \"Run All\" or \"Run all above\" found in the \"Cell\" dropdown menu.\n",
    "\n",
    "## Writing the report\n",
    "The report should be written within this notebook. We have prepared the last cell in this notebook for you where you should write the report. The report should contain 4 parts:\n",
    "\n",
    "* Name:\n",
    "* Introduction: A **few** sentences where you give a short introduction to the content and purpose of the exercise.\n",
    "* Answers to questions: For each of the questions provide an answer. It can be short answers or longer ones depending on the nature of the questions, but try to be efficient in your writing. (Don't include lots of program output or plots that aren't central to answering the question.)\n",
    "* Conclusion: Summarize your findings in a few sentences.\n",
    "\n",
    "It is important that you write the report in this last cell and **not** after each question! \n",
    "\n",
    "## Last but not least\n",
    "Have fun!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CellName: Init (#2)\n",
    "**CellType: Needed**  \n",
    "**Cell instruction:**\n",
    "\n",
    "In the cell below, we will import needed libraries. \n",
    "\n",
    "Run the cell by entering into the cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = 'cpu'\n",
    "# Uncomment this to use CUDA acceleration if available\n",
    "# device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"PyTorch: Using {device} device\")\n",
    "# The floating point data type can be changed here\n",
    "dtype_torch = torch.float32\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn, Tensor\n",
    "from collections import OrderedDict\n",
    "import torchmetrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# CellName: Data (#3)\n",
    "**CellType: Needed**  \n",
    "**Cell instruction:**\n",
    "\n",
    "This cell defines the two synthetic datasets. The last function is used for standardization of the data. \n",
    "\n",
    "Run the cell by entering into the cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syn2(N):\n",
    "    \"Generate data for a classification problem in 2D.\"\n",
    "    x = np.empty(shape=(N, 2))\n",
    "    d = np.empty(shape=(N, 1))\n",
    "    N1 = N // 2\n",
    "\n",
    "    # Positive samples\n",
    "    x[:N1,:] = 0.8 + np.random.normal(size=(N1, 2))\n",
    "    # Negative samples\n",
    "    x[N1:,:] = -.8 + np.random.normal(size=(N-N1, 2))\n",
    "\n",
    "    # Target\n",
    "    d[:N1] = 1\n",
    "    d[N1:] = 0\n",
    "\n",
    "    return x, d\n",
    "\n",
    "def regr1(N, periods=2, damp=False, v=0):\n",
    "    \"Generate data for 1D regression problem with damped cosine and noise\"\n",
    "    dx = 2*periods*np.pi / (N-1)\n",
    "    x = np.arange(N) * dx\n",
    "\n",
    "    if damp:\n",
    "        d = np.cos(x)*np.exp(-x*0.05)\n",
    "    else:\n",
    "        d = np.cos(x)\n",
    "    noise = lambda n : np.random.normal(size=n)\n",
    "    std_signal = np.std(d)\n",
    "    d = d + v * std_signal * noise(N)\n",
    "\n",
    "    return x[:, None], d[:, None]\n",
    "\n",
    "def standard(x):\n",
    "    \"Mean and stddev across samples\"\n",
    "    return np.mean(x, axis=0), np.std(x, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CellName: PlotData (#4)\n",
    "**CellType: Information**  \n",
    "**Cell instruction:**\n",
    "\n",
    "Here, we generate 100 cases for *syn2* and *regr1* datasets and plot them. \n",
    "\n",
    "Run the cell by entering into the cell and press \"CTRL Enter\". \n",
    "\n",
    "**Note!** This cell is not needed for the actual exercises, it is just to visualize the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = 0 means random, seed > 0 means fixed\n",
    "seed = 0\n",
    "np.random.seed(seed) if seed else None\n",
    "\n",
    "x, d = syn2(100)\n",
    "plt.figure()\n",
    "plt.scatter(x[:,0], x[:,1], c=d)\n",
    "\n",
    "# Regression, one period, no noise\n",
    "x, d = regr1(100, 2, False, 0)\n",
    "plt.figure()\n",
    "plt.scatter(x, d)\n",
    "\n",
    "# Regression, 1.5 period, exponential damping, some noise\n",
    "x, d = regr1(100, 3, True, 0.2)\n",
    "plt.figure()\n",
    "plt.scatter(x, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CellName: MLP (#5)\n",
    "**CellType: Needed**  \n",
    "**Cell instruction:**\n",
    "\n",
    "This cell defines the MLP model. Several MLP hyperparameters are needed to define a model.\n",
    "Here is a list of them:  \n",
    "(**Note:** They can all be specified when you call\n",
    "this function in later cells. The ones specified in this cell are the default values.)\n",
    "\n",
    "* inputs: the input dimension (integer)\n",
    "\n",
    "* output: the input dimension (integer)\n",
    "\n",
    "* nodes: size of the network, eg `[5]` for a one hidden layer with 5 nodes and `[5, 3]` for a two layer network with 5 and 3 hidden nodes each.\n",
    "\n",
    "* activation: the activation function. Most common are\n",
    "    * `nn.ReLU`\n",
    "    * `nn.Tanh`\n",
    "        \n",
    "* output_activation: the activation function for the output nodes. Most common are\n",
    "    * `None` (linear activation)\n",
    "    * `nn.Sigmoid`\n",
    "    * `nn.Softmax`\n",
    "      \n",
    "Run the cell by entering into the cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    \"A simple MLP with one or more fully connected layers\"\n",
    "\n",
    "    def __init__(self, *, inputs=1, outputs=1, nodes=[4], activation=nn.Tanh, out_activation=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs (int, optional): The number of input nodes.\n",
    "            outputs (int, optional): The number of output nodes.\n",
    "            nodes (list, optional): A list of layer sizes.\n",
    "            activation: Activation function (or None for linear). Defaults to nn.Tanh\n",
    "            out_activation (optional): Activation function for output layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        seqstack = OrderedDict()\n",
    "        prevn = inputs\n",
    "        for i, n in enumerate(nodes):\n",
    "            seqstack[f\"layer{i+1}\"] = nn.Linear(prevn, n, dtype=dtype_torch)\n",
    "            prevn = n\n",
    "            if activation is not None:\n",
    "                seqstack[f\"act{i+1}\"] = activation()\n",
    "        seqstack[\"layerN\"] = nn.Linear(prevn, outputs, dtype=dtype_torch)\n",
    "        if out_activation is not None:\n",
    "            seqstack[\"actN\"] = out_activation()\n",
    "        self.mlp_stack = nn.Sequential(seqstack)\n",
    "\n",
    "    def forward(self, x : Tensor):\n",
    "        \"Apply the network stack on some input\"\n",
    "        return self.mlp_stack(x)\n",
    "\n",
    "    def predict(self, input_data):\n",
    "        \"\"\"\n",
    "        Apply the network on a set of input data.\n",
    "\n",
    "        Args:\n",
    "            input_data (np.ndarray): Input data\n",
    "\n",
    "        Returns:\n",
    "            pred (np.ndarray): Predicted output.\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        inp = torch.tensor(input_data, dtype=dtype_torch, device=device)\n",
    "        with torch.no_grad():\n",
    "            pred = self(inp)\n",
    "        return pred.cpu().numpy()\n",
    "\n",
    "    def __str__(self):\n",
    "        s = super().__str__()\n",
    "        ps = [\"Named parameters:\"] + [\n",
    "            f\"{name}: {param.numel()}\" for name, param in\n",
    "             self.mlp_stack.named_parameters() if param.requires_grad]\n",
    "        totp = sum(p.numel() for p in self.mlp_stack.parameters() if p.requires_grad)\n",
    "        return s + f\"\\nTrainable parameters: {totp}\\n\" + \"\\n  \".join(ps) + \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CellName: Training (#6)\n",
    "**CellType: Needed**  \n",
    "**Cell Instruction:**\n",
    "\n",
    "This cell defines functions for training the model for a single epoch (`train_epoch`),\n",
    "evaluating the performance in the validation data (`test`) and training and validating over\n",
    "many epochs (`train_loop`). Finally, it defines a function (`plot_training`) for plotting\n",
    "the training progress.\n",
    "\n",
    "The `train_loop` function takes a previously defined `Network` model, two PyTorch `DataLoader`s\n",
    "that provide the data for training and test, and several hyperparameters:\n",
    "\n",
    "* loss_fn: The error function used during training. There are three common ones\n",
    "    * `nn.MSELoss` (mean squared error)\n",
    "    * `nn.BCELoss` (binary cross entropy)\n",
    "    * `nn.CrossEntropyLoss` (categorical cross entropy)\n",
    "\n",
    "* optimizer: The error minimization method, which is constructed with information about the model and a learning rate. Common choices are\n",
    "    * `torch.optim.SGD`\n",
    "    * `torch.optim.Adam`\n",
    "\n",
    "* metrics: Additional metrics to compute and print besides the loss. We use the [torcheval.metrics package](https://docs.pytorch.org/torcheval/main/torcheval.metrics.html) and pass the metric(s) as a dict with `{name: metric}`. Examples:\n",
    "    * `{'accuracy': ptmetrics.BinaryAccuracy()}`\n",
    "    * `{'MSE': ptmetrics.MeanSquaredError()}`\n",
    "\n",
    "Run the cell by entering into the cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(*, model : Network, dataloader : DataLoader,\n",
    "                loss_fn, optimizer : torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Train a model for a single epoch.\n",
    "\n",
    "    Args:\n",
    "        model (Network): The network.\n",
    "        dataloader (DataLoader): Batch DataLoader with training data.\n",
    "        loss_fn (Loss): Loss function, e.g. nn.MSELoss.\n",
    "        optimizer (Optimizer): The optimizer used to update the network.\n",
    "\n",
    "    Returns:\n",
    "        train_loss (float): Training error over all batches.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for X, y in dataloader:\n",
    "        X, y = X.to(device), y.to(device)   # Move data to GPU if necessary\n",
    "        optimizer.zero_grad()   # Reset the gradients\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        train_loss += loss.item() * len(X)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return train_loss / len(dataloader.dataset)\n",
    "\n",
    "def test(*, model : Network, dataloader : DataLoader, loss_fn, metrics=[]):\n",
    "    \"\"\"\n",
    "    Test a model on a set of data.\n",
    "\n",
    "    Args:\n",
    "        model (Network): The network.\n",
    "        dataloader (DataLoader): DataLoader with data to test.\n",
    "        loss_fn (Loss): Loss function, e.g. nn.MSELoss.\n",
    "        metrics (iterable): Additional metrics from torchmetrics.\n",
    "\n",
    "    Returns:\n",
    "        loss (float): Mean error over all batches.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            loss += loss_fn(pred, y).item() * len(X)\n",
    "            for m in metrics:\n",
    "                m.update(pred, y)\n",
    "    return loss / len(dataloader.dataset)\n",
    "\n",
    "\n",
    "def train_loop(*, model : Network, train_dataloader : DataLoader,\n",
    "               val_dataloader : DataLoader = None, loss_fn,\n",
    "               optimizer : torch.optim.Optimizer, epochs : int,\n",
    "               print_every:int = 100, metrics=None, print_final=True):\n",
    "    \"\"\"\n",
    "    Train and optionally test a model.\n",
    "\n",
    "    Args:\n",
    "        model (Network): The network.\n",
    "        train_dataloader (DataLoader): Training data.\n",
    "        val_dataloader (DataLoader, optional): Validation data.\n",
    "        loss_fn (Loss): Loss function, e.g. nn.MSELoss.\n",
    "        optimizer (Optimizer): An optimizer from torch.optim.\n",
    "        epochs (int): Number of epochs to train for.\n",
    "        print_every (int, optional): Print loss every so many epochs. Defaults to 100.\n",
    "        metrics (dict(name: metric), optional): Record/print these additional metrics.\n",
    "        print_final(bool, optional): Print final metrics. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        train_losses (list(float)): Training loss during each epoch.\n",
    "        val_losses (list(float)): Validation loss after each epoch.\n",
    "        metrics_res (dict(name: list(float))): Values of metrics after each epoch.\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_loss = np.nan\n",
    "\n",
    "    # Move metrics to CPU/GPU and prepare for their output\n",
    "    metrics = {name: m.to(device) for name, m in (metrics or {}).items()}\n",
    "    metrics_res = {name: [] for name in metrics.keys()}\n",
    "\n",
    "    for t in range(epochs):\n",
    "        train_loss = train_epoch(model=model, dataloader=train_dataloader,\n",
    "                           loss_fn=loss_fn, optimizer=optimizer)\n",
    "        train_losses.append(train_loss)\n",
    "        if val_dataloader is not None:\n",
    "            for m in metrics.values():\n",
    "                m.reset()\n",
    "            val_loss = test(dataloader=val_dataloader, model=model,\n",
    "                            loss_fn=loss_fn, metrics=metrics.values())\n",
    "            val_losses.append(val_loss)\n",
    "            for name, m in metrics.items():\n",
    "                metrics_res[name].append(m.compute().cpu())\n",
    "        if (print_every > 0 and t % print_every == 0) or (\n",
    "                print_every >= 0 and t + 1 == epochs):\n",
    "            extras = [f\" {n} {v[-1]:<7f}\" if torch.isreal(v[-1])\n",
    "                      else f\" {n} {v[-1]}\"\n",
    "                      for n, v in metrics_res.items()]\n",
    "            print(f\"Epoch {t+1:<7d} train {train_loss:<7f} \"\n",
    "                  f\" validation {val_loss:<7f}\", \"\".join(extras))\n",
    "    if print_final:\n",
    "        print(\"\\n** Validation metrics after training **\\n\"\n",
    "              f\"Loss {val_losses[-1]:<7g}\")\n",
    "        for n, v in metrics_res.items():\n",
    "            if torch.isreal(v[-1]):\n",
    "                print(f\"{n} {v[-1]:<7g}\")\n",
    "            else:\n",
    "                print(f\"{n}:\")\n",
    "                print(v[-1])\n",
    "        print()\n",
    "    return train_losses, val_losses, metrics_res\n",
    "\n",
    "def plot_training(train_loss, val_loss, metrics_res={}):\n",
    "    \"Plot the training history\"\n",
    "    plt.figure()\n",
    "    plt.ylabel('Loss / Metric')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.plot(train_loss, label=\"Training loss\")\n",
    "    plt.plot(val_loss, label=\"Validation loss\")\n",
    "    for name, res in metrics_res.items():\n",
    "        if torch.isreal(res[0]):\n",
    "            plt.plot(res, label=name)\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CellName: Boundary (#7)\n",
    "**CellType: Needed**  \n",
    "**Cell Instruction:**\n",
    "\n",
    "This cell defines a function for presenting the output of binary MLP classifiers, plotting the decision boundary for a problem with 2D input. In brief, this function defines a grid that covers the input data. Each grid point is then used as an input to the trained MLP and to compute an output. If the output is close to 0.5 it is marked as the boundary.\n",
    "\n",
    "Run the cell by entering into the cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_boundary(X : np.ndarray, Y1 : np.ndarray, model):\n",
    "    \"\"\"\n",
    "        Plot classfication and decision boundary for binary classification problem\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): input\n",
    "            Y1 (np.ndarray): target\n",
    "            model (Network): the model\n",
    "\n",
    "        Returns:\n",
    "            None.\n",
    "    \"\"\"\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    # grid stepsize\n",
    "    h = 0.025\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    Z[Z > .5] = 1\n",
    "    Z[Z <= .5] = 0\n",
    "\n",
    "    Y_pr = model.predict(X).flatten()\n",
    "    Y = Y1.flatten()\n",
    "\n",
    "    Y_pr[Y_pr > .5] = 1\n",
    "    Y_pr[Y_pr <= .5] = 0\n",
    "    Y[(Y != Y_pr) & (Y == 0)] = 2\n",
    "    Y[(Y != Y_pr) & (Y == 1)] = 3\n",
    "\n",
    "    plt.figure()\n",
    "    #plt.contourf(xx, yy, Z, cmap=plt.cm.PRGn, alpha = .9)\n",
    "    plt.contour(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "    plt.scatter(X[Y == 1, 0], X[Y == 1, 1], marker='+', c='k')\n",
    "    plt.scatter(X[Y == 0, 0], X[Y == 0, 1], marker='o', c='k')\n",
    "\n",
    "    plt.scatter(X[Y == 3, 0], X[Y == 3, 1], marker = '+', c='r')\n",
    "    plt.scatter(X[Y == 2, 0], X[Y == 2, 1], marker = 'o', c='r')\n",
    "\n",
    "    plt.ylabel('x2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "End of \"Needed\" and \"Information\" cells. Below are the cells for the actual exercise.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CellName: Ex1 (#8)\n",
    "**CellType: Exercise**  \n",
    "**Cell instruction:**\n",
    "\n",
    "Questions 1-4 look at three essential parameters that controls the training process of an MLP: the *learning rate*, *batch size* and *number of epochs* (or epochs for short). By training process we mean here the minimization of the given loss function. The task is to train an MLP with four hidden nodes that can fit the *regr1* dataset. In this version of the dataset, there is no noise. Therefore, we need not specify any seed.\n",
    "\n",
    "The dataset and network have been selected so that it is possible, but not trivial, to get a good training result.\n",
    "A successful training means here when the networks has reached a loss < 0.01, and visually have fitted the data accurately. In this exercise we do not care about possible overfitting, only about the minimization of the loss function, we therefore do not have a validation dataset.\n",
    "\n",
    "## Question 1, variations in pre-defined MLP\n",
    "For the first question you can simply run the cell below. It will load 50 samples from the *regr1* dataset (no noise added). The network has 4 hidden nodes in a single hidden layer, *tanh* activation function, linear output activation function, *stochastic gradient descent* as minimization method, MSE loss function, and a learning rate of 0.05.\n",
    "It will train for 4000 epochs using a batchsize of 50, meaning that we efficiently are using ordinary gradient descent learning. Run this cell five times. **(a) Do you see the same loss vs epoch behavior each time your run?** If not, **why?** **(b) Do you observe that training fails, i.e. do not reach low loss, during any of these five runs?** \n",
    "\n",
    "## Question 2, vary learning rate\n",
    "You will now study what happens when you train with different learning rates. Test at least 5 different learning rates in the range from 0.001 to 0.5. For each learning rate train the network three times and record the average MSE value over these three runs. **Present your average MSE results and discuss your findings**.\n",
    "\n",
    "**Note:** You should keep the same settings as for Q1, only vary the learning rate. The learning rate is best investigated with (roughly) proportional changes rather than constant steps: For example, trying 0.5, 0.2, 0.1, 0.05, 0.02 etc. typically gives more interesting results than 0.5, 0.4, 0.3, 0.2, 0.1.\n",
    "\n",
    "## Question 3, vary (mini)batch size\n",
    "We now (hopefully) have discovered that the learning rate influences the efficiency of the loss minimization. We will now look at what happens when we use *stochastic gradient descent*, meaning that we will have a \"batch size\" that is smaller the the size of the training data. (We now adapt to the language of ANN packages such as PyTorch and use the word \"batch\" where most literature would use \"mini-batch\".) Use a fixed learning rate of 0.05, but test different batch sizes in the range 1 to 50. Train three different networks for each batch size, but this time record if the training was successful (i.e. MSE < 0.01) and approximately after how many epochs the good solution was found. **Present and discuss your findings**.\n",
    "\n",
    "**Note:** The batch size should (fairly well) divide the total data size. With a data size of 50, good batch sizes are 50, 25, 13, 10, 5... Sizes that fit poorly, like 40, create a small last batch which could give you the disadvantages of both small and large batch sizes at the same time.\n",
    "\n",
    "## Question 4, select good hyper-parameters\n",
    "Find a combination of learning rate and batch size that gives a good solution within 1000 epochs. We always have to remember that two runs with identical hyper parameters (e.g. learning rate, batch size etc) will result in different final results. Your set of parameters should *most* of the times result in a good solution within 1000 epochs. **Present your best combination of learning rate and batch size, and its result**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Generate training data\n",
    "x_trn, d_trn = regr1(50, 2, 0, 0.0)\n",
    "\n",
    "# Standardization of inputs\n",
    "mu, std = standard(x_trn)\n",
    "x_trn = (x_trn - mu) / std\n",
    "\n",
    "# Define the network, cost function and training settings\n",
    "model_ex1 = Network(\n",
    "    inputs=1,            # number of input nodes\n",
    "    outputs=1,           # number of output nodes\n",
    "    nodes=[4],           # number of nodes in hidden layer\n",
    "    activation=nn.Tanh,  # activation function in hidden layer\n",
    "    out_activation=None  # activation function in output layer (if not linear)\n",
    "    ).to(device)         # move data to GPU or keep with CPU\n",
    "\n",
    "# Optimization parameters\n",
    "opt_method = torch.optim.SGD  # minimization method\n",
    "learning_rate = 0.05          # learning rate\n",
    "loss_fn = nn.MSELoss()        # loss function, MSE\n",
    "number_epochs = 4000\n",
    "minibatch_size = 50\n",
    "\n",
    "# Additional metrics to print\n",
    "metrics = {'MSE': torchmetrics.MeanSquaredError()}\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = opt_method(model_ex1.parameters(), lr=learning_rate)\n",
    "\n",
    "# Print a summary of the model\n",
    "print(model_ex1)\n",
    "\n",
    "# Turn the training data into a dataset with Tensors on the GPU or CPU\n",
    "dset_trn = TensorDataset(torch.tensor(x_trn, device=device, dtype=dtype_torch),\n",
    "                         torch.tensor(d_trn, device=device, dtype=dtype_torch))\n",
    "\n",
    "# Create a batch loader for the training data\n",
    "dl_trn = DataLoader(dset_trn, batch_size=minibatch_size)\n",
    "\n",
    "# Train the network and print the progress\n",
    "train_loss, val_loss, metrics_res = train_loop(\n",
    "    model=model_ex1,\n",
    "    train_dataloader=dl_trn,\n",
    "    val_dataloader=dl_trn, # Test with the training data\n",
    "    loss_fn=loss_fn,\n",
    "    metrics=metrics,\n",
    "    optimizer=optimizer,\n",
    "    print_every=100,\n",
    "    epochs=number_epochs)\n",
    "\n",
    "# Plot the training history\n",
    "plot_training(train_loss, val_loss, metrics_res)\n",
    "\n",
    "# Predict output on the training data\n",
    "d_pred = model_ex1.predict(x_trn)\n",
    "\n",
    "# Plot the result\n",
    "plt.figure()\n",
    "plt.ylabel('Prediction / Target')\n",
    "plt.xlabel('Input')\n",
    "plt.scatter(x_trn, d_trn, label='Target')\n",
    "plt.scatter(x_trn, d_pred, label='Prediction')\n",
    "plt.title('Prediction vs Target')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CellName: Ex2 (#9)\n",
    "**CellType: Exercise**  \n",
    "**Cell instruction:**  \n",
    "\n",
    "The amount of weights in the network can also influence how long time we need to train, and of course if the problem itself is complex or not. The following two questions will highlight this.\n",
    "\n",
    "## Question 5, vary epochs\n",
    "The example below will load a slightly more complex *regr1* problem (an additional quarter of a period). We will use 10 hidden nodes for this problem. Use your optimal set of learning rate and batch size as found in Q4 and train the network below. **Compare the number of epochs needed to reach a good solution with that of Q4**. Note, you may need to vary the number of epochs a lot! If you cannot find a good solution in a reasonable number of epochs, you can \"revert\" the problem: optimize learning rate and batch size for Q5, and the see how those hyper-parameters perform on Q4.\n",
    "\n",
    "## Question 6, vary network size and other hyper-parameters\n",
    "Use the following line to load the *regr1* data set:\n",
    "\n",
    "`x_trn, d_trn = regr1(75, 5, 1, 0.0)`\n",
    "\n",
    "This will create an even more challenging regression task that may need an even larger network. Your task is to find a set of hyper-parameters (learning rate, batch size, epochs, 'size of the network') that result in a good solution. You can use more than one hidden layer for this task (if you want). To create many hidden layers, add many numbers to the `layers` list, for example: `layers = [10, 5, 5]`. **Present your set of good hyper-parameters and the result**. \n",
    "\n",
    "**Note:** If you cannot solve this task in *reasonable* time, present your best attempt!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# seed = 0 means random, seed > 0 means fixed\n",
    "seed = 0\n",
    "np.random.seed(seed) if seed else None\n",
    "\n",
    "# Generate training data\n",
    "# For Q5:\n",
    "x_trn, d_trn = regr1(50, 2.5, 0, 0.0)\n",
    "\n",
    "# For Q6:\n",
    "#x_trn, d_trn = regr1(75, 5, 1, 0.0)\n",
    "\n",
    "# Standardization of inputs\n",
    "mu, std = standard(x_trn)\n",
    "x_trn = (x_trn - mu) / std\n",
    "\n",
    "# Define the network, cost function and training settings\n",
    "model_ex2 = Network(\n",
    "    inputs=1,            # number of input nodes\n",
    "    outputs=1,           # number of output nodes\n",
    "    nodes=[10],           # number of nodes in hidden layer\n",
    "    activation=nn.Tanh,  # activation function in hidden layer\n",
    "    out_activation=None  # activation function in output layer (if not linear)\n",
    "    ).to(device)         # move data to GPU or keep with CPU\n",
    "\n",
    "# Optimization parameters\n",
    "opt_method = torch.optim.SGD  # minimization method\n",
    "learning_rate = 0.05          # learning rate\n",
    "loss_fn = nn.MSELoss()        # loss function, MSE\n",
    "number_epochs = 1000\n",
    "minibatch_size = 50\n",
    "\n",
    "# Additional metrics to print\n",
    "metrics = {'MSE': torchmetrics.MeanSquaredError()}\n",
    "\n",
    "# Print a summary of the model\n",
    "print(model_ex2)\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = opt_method(model_ex2.parameters(), lr=learning_rate)\n",
    "\n",
    "# Turn the training data into a dataset with Tensors on the GPU or CPU\n",
    "dset_trn = TensorDataset(torch.tensor(x_trn, device=device, dtype=dtype_torch),\n",
    "                         torch.tensor(d_trn, device=device, dtype=dtype_torch))\n",
    "\n",
    "# Create a batch loader for the training data\n",
    "dl_trn = DataLoader(dset_trn, batch_size=minibatch_size)\n",
    "\n",
    "# Train the network and print the progress\n",
    "train_loss, val_loss, metrics_res = train_loop(\n",
    "    model=model_ex2,\n",
    "    train_dataloader=dl_trn,\n",
    "    val_dataloader=dl_trn, # Test with the training data\n",
    "    loss_fn=loss_fn,\n",
    "    metrics=metrics,\n",
    "    optimizer=optimizer,\n",
    "    print_every=100,\n",
    "    epochs=number_epochs)\n",
    "\n",
    "# Plot the training history\n",
    "plot_training(train_loss, val_loss, metrics_res)\n",
    "\n",
    "# Predict output on the training data\n",
    "d_pred = model_ex2.predict(x_trn)\n",
    "\n",
    "# Plot the result\n",
    "plt.figure()\n",
    "plt.ylabel('Prediction / Target')\n",
    "plt.xlabel('Input')\n",
    "plt.scatter(x_trn, d_trn, label='Target')\n",
    "plt.scatter(x_trn, d_pred, label='Prediction')\n",
    "plt.title('Prediction vs Target')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CellName: Ex3 (#10)\n",
    "**CellType: Exercise**  \n",
    "**Cell instruction:**  \n",
    "\n",
    "We will now look at the classification problem defined by the *syn2* dataset.\n",
    "The cell below defines a single hidden node MLP. With this network you can only implement a linear decision boundary. Run the cell below to look at the resulting boundary that the MLP learns. The training accuracy is around 87-93%, because the data is generated randomly each time you run the code. \n",
    "\n",
    "## Question 7, optimize hyper-parameters for classification\n",
    "Your task is now to reach a larger accuracy by fitting a model with more hidden nodes (and possibly more hidden layers). \n",
    "Your aim is to reach a training accuracy > 95%. To do that you need to tune the learning rate, batch size, epochs and the size of your MLP. **Present your set of hyper parameters that reach > 95% accuracy**\n",
    "\n",
    "**Note**: To always generate exactly the same dataset each time you run the code you can set the *seed* to a value > 0. \n",
    "\n",
    "## Question 8, change learning algorithm\n",
    "We have so far only used stochastic gradient descent (SGD), but we know that there are modifications of SGD that are very popular, e.g. Adam.\n",
    "**Try the Adam optimizer for Q7, and compare (qualitatively) the results and the number of epochs needed to get them.**\n",
    "\n",
    "The interpretation of the learning rate differs a bit between SGD and Adam. Since your learning rate was optimized for SGD in Q7, you could consider optimizing it again for Adam, before you compare SGD with Adam. **Present changes you needed to make to improve the results of the Adam optimizer, if any.**\n",
    "\n",
    "**Info**: Adam has two extra parameters. The way we call the Adam optimizer, they will be kept at their default values *beta1* = 0.9 and *beta2* = 0.999. \n",
    "\n",
    "## Bonus tasks\n",
    "The bonus tasks are provided if you have extra time and want to continue to explore methods that can further enhance the minimization of the loss function. **These tasks are not required for the course and do not influence any grading**. \n",
    "\n",
    "The tasks listed below also mean that you have to change the code in code cell *Training* (#6). There will be links to appropriate documentation below.\n",
    "\n",
    "* Go back to Q7 and add use a momentum add-on to SGD. **Does momentum help?** (See documentation [here](https://docs.pytorch.org/docs/stable/generated/torch.optim.SGD.html))\n",
    "* It is common to also introduce a mechanism that can lower the learning rate as we train. If we are using stochastic gradient descent the mini-batch gradients will never be zero, meaning that we will always make some small weight updates. PyTorch has methods that can lower the learning rate as we train (see [here](https://docs.pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)). Again go back to Q7 and now use an exponentially decaying learning rate (`ExponentialLR`). **Does it help?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# seed = 0 means random, seed > 0 means fixed\n",
    "seed = 0\n",
    "np.random.seed(seed) if seed else None\n",
    "\n",
    "# Generate training data\n",
    "x_trn, d_trn = syn2(100)\n",
    "\n",
    "# General standardization of input data\n",
    "mu, std = standard(x_trn)\n",
    "x_trn = (x_trn - mu) / std\n",
    "\n",
    "# Define the network, cost function and training settings\n",
    "model_ex3 = Network(\n",
    "    inputs=x_trn.shape[1],      # number of input nodes\n",
    "    outputs=1,                  # number of output nodes\n",
    "    nodes=[1],                 # number of nodes in hidden layer\n",
    "    activation=nn.Tanh,         # activation function in hidden layer\n",
    "    out_activation=nn.Sigmoid   # activation function in output layer\n",
    "    ).to(device)                # move data to GPU or keep with CPU\n",
    "\n",
    "# Optimization parameters\n",
    "opt_method = torch.optim.SGD    # minimization method\n",
    "learning_rate = 0.05             # learning rate\n",
    "loss_fn = nn.BCELoss()          # loss function, binary cross entropy\n",
    "number_epochs = 1000\n",
    "minibatch_size = 100\n",
    "\n",
    "# Additional metrics to print\n",
    "metrics = { 'accuracy': torchmetrics.Accuracy('binary') }\n",
    "\n",
    "# Print a summary of the model\n",
    "print(model_ex3)\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = opt_method(model_ex3.parameters(), lr=learning_rate)\n",
    "\n",
    "# Turn the training data into a dataset with Tensors on the GPU or CPU\n",
    "dset_trn = TensorDataset(torch.tensor(x_trn, device=device, dtype=dtype_torch),\n",
    "                         torch.tensor(d_trn, device=device, dtype=dtype_torch))\n",
    "\n",
    "# Create a batch loader for the training data\n",
    "dl_trn = DataLoader(dset_trn, batch_size=minibatch_size)\n",
    "\n",
    "# Train the network and print the progress\n",
    "train_loss, val_loss, metrics_res = train_loop(\n",
    "    model=model_ex3,\n",
    "    train_dataloader=dl_trn,\n",
    "    val_dataloader=dl_trn, # Test with the training data\n",
    "    loss_fn=loss_fn,\n",
    "    metrics=metrics,\n",
    "    optimizer=optimizer,\n",
    "    print_every=100,\n",
    "    epochs=number_epochs)\n",
    "\n",
    "# Plot the training history\n",
    "plot_training(train_loss, val_loss, metrics_res)\n",
    "\n",
    "# Predict output on the training data\n",
    "d_pred = model_ex3.predict(x_trn)\n",
    "\n",
    "# Plot the decision boundary\n",
    "decision_boundary(x_trn, d_trn, model_ex3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# The report!\n",
    "\n",
    "We have added intructions inside this report template. As you write your report, remove these instructions.\n",
    "\n",
    "## Your name\n",
    "\n",
    "## Introduction\n",
    "A few sentences about the overall theme of the exercise.\n",
    "\n",
    "## Answers to questions\n",
    "Provide enough information to clarify the meaning of your answers, so that they can be understood by someone who does not scroll up and read the entire instruction.\n",
    "\n",
    "The questions are repeated here, for clarity of what is demanded. If it does not fit your style to quote them verbatim, change the format. \n",
    "\n",
    "**Question 1**, variations in pre-defined MLP  \n",
    "**(a)** Do you see the same loss vs epoch behavior each time your run? If not, why?  \n",
    "**(b)** Do you observe that training fails, i.e. do not reach low loss, during any of these five runs? \n",
    "\n",
    "**Question 2**, vary learning rate  \n",
    "Present your average MSE results and discuss your findings.\n",
    "\n",
    "**Question 3**, vary (mini)batch size  \n",
    "Present and discuss your findings.\n",
    "\n",
    "**Question 4**, select good hyper-parameters  \n",
    "Present your best combination of learning rate and batch size, and its result.\n",
    "\n",
    "**Question 5**, vary epochs  \n",
    "Compare the number of epochs needed to reach a good solution with that of Q4.  \n",
    "\n",
    "Note: If you cannot find a good solution in a reasonable number of epochs, you can \"revert\" the problem: optimize learning rate and batch size for Q5, and the see how those hyper-parameters perform on Q4.\n",
    "\n",
    "**Question 6**, vary network size and other hyper-parameters  \n",
    "Present your set of good hyper-parameters and the result. \n",
    "\n",
    "Note: If you cannot solve this task in *reasonable* time, present your best attempt!\n",
    "\n",
    "**Question 7**, optimize hyper-parameters for classification  \n",
    "Present your set of hyper-parameters that reach > 95% accuracy\n",
    "\n",
    "**Question 8**, change learning algorithm  \n",
    "Try the Adam optimizer for Q7, and compare (qualitatively) the results and the number of epochs needed to get them. Present changes you needed to make to improve the results of the Adam optimizer, if any.\n",
    "\n",
    "**Bonus tasks** (if you feel inspired)\n",
    "\n",
    "## Summary\n",
    "Connect the summary to your introduction, to provide a brief overview of your findings.\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "530px",
    "width": "356.167px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
