{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"font-size:40px;\"><center>Exercise II:<br> Model selection with MLPs\n",
    "</center></h1>\n",
    "\n",
    "# Introduction\n",
    "## Short summary\n",
    "In this exercise you will: \n",
    "\n",
    "* train multi-layer perceptrons (MLPs) for both binary and multiple classification problems and a regression problem, and perform model selection to optimize validation performance\n",
    "\n",
    "You should write the report of the exercise within this notebook. The details of how to do that can be found below in section \"Writing the report\".\n",
    "\n",
    "**Deadline for submitting the report: See Canvas assignment.**\n",
    "\n",
    "## The data\n",
    "There are several datasets in this exercise. \n",
    "\n",
    "### syn2\n",
    "The *syn2* dataset represents a binary classification problem. The input data is 2D which allows for an easy visual inspection of the different classes and the decision boundary implemented by the network. The dataset is generated using random numbers each time you run the cell. This means that each time you generate the data it will be slightly different. You can control this by having a fixed *seed* to the random number generator. The cell \"PlotData\" will plot the *syn2* dataset.\n",
    "\n",
    "Note: This is the same dataset as in exercise 1.\n",
    "\n",
    "### regr2\n",
    "There *regr2* dataset represents a more complex synthetic regression problem than *regr1* from exercise 1. It has 6 inputs (independent variables) and one target variable (dependent variable). It is generated according to the following formula:  \n",
    "\n",
    "$\\qquad d = 2x_1 + x_2x_3^2 + e^{x_4} + 5x_5x_6 + 3\\sin(2\\pi x_6) + \\alpha\\epsilon$  \n",
    "    \n",
    "where $\\epsilon$ is added normally distributed noise and $\\alpha$ is a parameter controlling the size of the added noise. Variables $x_1,...,x_4$ are normally distributed with zero mean and unit variance, whereas $x_5, x_6$ are uniformly distributed ($[0,1]$). The target value $d$ has a non-linear dependence on ***x***.\n",
    "\n",
    "### Spiral data\n",
    "This is the \"famous\" spiral dataset that consists of two 2-D spirals, one for each class. The perfect classification boundary is also a spiral. The cell \"PlotData\" will plot this dataset.\n",
    "\n",
    "### Japanese vowels dataset\n",
    "*This data set is taken from the UCI Machine Learning Repository https://archive.ics.uci.edu/ml/datasets/Japanese+Vowels* In short, nine male speakers uttered two Japanese vowels /ae/ successively. For each utterance, a discrete times series was produced where each time point consists of 12 (LPC cepstrum) coefficients. The length of each time series was between 7-29. \n",
    "Here we treat each point of the time series as a feature (12 inputs). In total we have 9961\n",
    "data points which then has been divided into 4274 for training, 2275 for validation and 3412 for test. The original data files are provided as *ae.train* and *ae.test*. The task is now based on a single sample value of one of the speakers, determine which speaker it was. This is, in summary, a 9-class classification problem with 12 input values for each case.\n",
    "\n",
    "### Bioconcentration dataset\n",
    "*This data set is taken from the UCI Machine Learning Repository https://archive.ics.uci.edu/ml/datasets/QSAR+Bioconcentration+classes+dataset* In short, this is a dataset of manually-curated bioconcentration factors (BCF) for 779 chemicals used to determine the mechanisms of bioconcentration, i.e. to predict whether a chemical: (1) is mainly stored within lipid tissues, (2) has additional storage sites (e.g. proteins), or (3) is metabolized/eliminated. Data were randomly split into a training set of 584 compounds (75%) and a test set of 195 compounds (25%), preserving the proportion between the classes. The independent variables consist of 9 molecular descriptors. This is, in summary, a 3-class classification problem with 9 input values for each case.\n",
    "\n",
    "## The exercises\n",
    "There are 10 questions in this exercise, in five different cells below. \n",
    "\n",
    "For questions 1-6, code is available that you can run directly or only need to make small modifications to. The first 3 questions deal with 2D binary classification problems. Here you will be able to see the boundary implemented by the different MLPs that you train. Questions 4-6 deal with training a regression network for the *regr2* dataset.\n",
    "\n",
    "For questions 7-10 we only provide parts of the code and you should add the rest. However, it is typically just a matter of paste and copy from the previous code cells (in a proper way). Question 7-8 deals with Japanese vowels classification problem; here your task is to come up with a model that optimizes the validation result. Question 9 is about the Bioconcentration dataset, and again you should come up with a good model. Finally, the last question is to find a model that can solve the spiral problem.\n",
    "\n",
    "## The different 'Cells'\n",
    "This notebook contains several cells with python code, together with the markdown cells (like this one) with only text. Each of the cells with python code has a \"header\" markdown cell with information about the code. The table below provides a short overview of the code cells. \n",
    "\n",
    "| #  |  CellName | CellType | Comment |\n",
    "| :--- | :-------- | :-------- | :------- |\n",
    "| 1 | Init | Needed | Sets up the environment|\n",
    "| 2 | Data | Needed | Defines the functions to generate the artificial datasets |\n",
    "| 3 | PlotData | Information | Plots the 2D classification datasets |\n",
    "| 4 | Confusion | Needed | Functions that plots the confusion matrix |\n",
    "| 5 | MLP | Needed | Defines the MLP model |\n",
    "| 6 | Boundary | Needed | Function that can show 2D classification boundaries |\n",
    "| 7 | Statistics | Needed | Functions that calculate and plot performance measures |\n",
    "| 8 | Training | Needed | Functions for training and testing the MLP model |\n",
    "| 9 | Ex1 | Exercise | For question 1-3 (syn2) |\n",
    "| 10 | Ex2 | Exercise | For question 4-6 (regr2) |\n",
    "| 11 | Ex3 | Exercise | For question 7-8 (vowels) |\n",
    "| 12 | Ex4 | Exercise | For question 9 (bcf) |\n",
    "| 13 | Ex5 | Exercise | For question 10 (twospirals) |\n",
    "\n",
    "In order for you to start with the exercise you need to run all cells with the celltype \"Needed\". The very first time you start with this exercise we suggest that you enter each of the needed cells, read the cell instruction and run the cell. It is important that you do this in the correct order, starting from the top and work you way down the cells. Later when you have started to work with the notebook it may be easier to use the command \"Run All\" or \"Run all above\" found in the \"Cell\" dropdown menu.\n",
    "\n",
    "## Writing the report\n",
    "First the report should be written within this notebook. We have prepared the last cell in this notebook for you where you should write the report. The report should contain 4 parts:\n",
    "\n",
    "* Name:\n",
    "* Introduction: A **few** sentences where you give a short introduction to the content and purpose of the exercise.\n",
    "* Answers to questions: For each of the questions provide an answer. It can be short answers or longer ones depending on the nature of the questions, but try to be efficient in your writing. (Don't include lots of program output or plots that aren't central to answering the question.)\n",
    "* Conclusion: Summarize your findings in a few sentences.\n",
    "\n",
    "It is important that you write the report in this last cell and **not** after each question! \n",
    "\n",
    "## Last but not least\n",
    "Have fun!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# CellName: Init (#1)\n",
    "**CellType: Needed**  \n",
    "**Cell instruction: Initializing the libraries**\n",
    "\n",
    "In the cell below, we import all the libraries that are needed for this exercises. \n",
    "\n",
    "Run the cell by entering into the cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = 'cpu'\n",
    "# Uncomment this to use CUDA acceleration if available\n",
    "# device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"PyTorch: Using {device} device\")\n",
    "# The floating point data type can be changed here\n",
    "dtype_torch = torch.float32\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "import torchmetrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, log_loss, classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CellName: Data (#2)\n",
    "**CellType: Needed**  \n",
    "**Cell instruction: Defining synthetic data sets**\n",
    "\n",
    "This cell defines the different synthetic data sets. It also provides functions for reading the Vowels dataset, the Bioconcentration dataset and the Spiral data. The last function is used for standardization of the data. \n",
    "\n",
    "Run the cell by entering into the cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syn2(N):\n",
    "    \"Generate data for classification problem in 2D.\"\n",
    "    x = np.empty(shape=(N, 2))\n",
    "    d = np.empty(shape=(N, 1))\n",
    "    N1 = N // 2\n",
    "\n",
    "    # Positive samples\n",
    "    x[:N1,:] = 0.8 + np.random.normal(size=(N1, 2))\n",
    "    # Negative samples\n",
    "    x[N1:,:] = -.8 + np.random.normal(size=(N-N1, 2))\n",
    "\n",
    "    # Target\n",
    "    d[:N1] = 1\n",
    "    d[N1:] = 0\n",
    "\n",
    "    return x, d\n",
    "\n",
    "def regr2(N, v=0):\n",
    "    \"Generate more complicated regression data.\"\n",
    "    x = np.empty(shape=(N, 6))\n",
    "\n",
    "    uni = lambda n : np.random.uniform(0, 1, n)\n",
    "    norm = lambda n : np.random.normal(0, 1, n)\n",
    "    noise = lambda n : np.random.normal(0, 1, n)\n",
    "\n",
    "    x[:, :4] = norm((N, 4))\n",
    "    x[:, 4:] = uni((N, 2))\n",
    "\n",
    "    d = (2*x[:, 0] + x[:, 1]*x[:, 2]**2 + np.exp(x[:, 3]) +\n",
    "         5*x[:, 4]*x[:, 5] + 3*np.sin(2*np.pi*x[:, 5]))\n",
    "    std_signal = np.std(d)\n",
    "    d = d + v * std_signal * noise(N)\n",
    "\n",
    "    return x, d[:, None]\n",
    "\n",
    "def twospirals(n_points, turns = 3, noise = 0.5):\n",
    "    \"Generate the two spirals dataset.\"\n",
    "    n = (np.random.rand(n_points, 1) * 0.95 + 0.05) * turns * (2*np.pi)\n",
    "    d1x = -np.cos(n)*n + np.random.rand(n_points, 1) * noise\n",
    "    d1y = np.sin(n)*n + np.random.rand(n_points, 1) * noise\n",
    "    return (np.vstack((np.hstack((d1x, d1y)), np.hstack((-d1x, -d1y)))),\n",
    "            np.hstack((np.zeros(n_points), np.ones(n_points)))[:, None])\n",
    "\n",
    "def vowels():\n",
    "    \"Load and prepare the Japanese vowels dataset.\"\n",
    "    def pre_proc(file_name):\n",
    "        block = []\n",
    "        x = []\n",
    "\n",
    "        with open(file_name) as file:\n",
    "            for line in file:\n",
    "                if line.strip():\n",
    "                    numbers = [float(n) for n in line.split()]\n",
    "                    block.append(numbers)\n",
    "                else:\n",
    "                    x.append(block)\n",
    "                    block = []\n",
    "        x = [torch.Tensor(ar) for ar in x]\n",
    "        return x\n",
    "\n",
    "    x_trn = pre_proc('ae.train')\n",
    "    x_tst = pre_proc('ae.test')\n",
    "\n",
    "    # Labels\n",
    "    chunk1 = list(range(30, 270, 30))\n",
    "    d_trn = []\n",
    "    person = 0\n",
    "\n",
    "    for i, block in enumerate(x_trn):\n",
    "        if i in chunk1:\n",
    "            person += 1\n",
    "        d_trn.extend([person] * block.shape[0])\n",
    "\n",
    "    chunk2 = [31, 35, 88, 44, 29, 24, 40, 50, 29]\n",
    "    chunk2 = np.cumsum(chunk2)\n",
    "    d_tst = []\n",
    "    person = 0\n",
    "    for i, block in enumerate(x_tst):\n",
    "        if i in chunk2:\n",
    "            person += 1\n",
    "        d_tst.extend([person] * block.shape[0])\n",
    "\n",
    "    x_trn = np.vstack(x_trn)\n",
    "    x_tst = np.vstack(x_tst)\n",
    "    # Make 1-hot arrays\n",
    "    num_classes = 9\n",
    "    classes = np.eye(num_classes, dtype=np.float32)\n",
    "    d_trn = classes[d_trn]\n",
    "    d_tst = classes[d_tst]\n",
    "\n",
    "    # Split into train, test and validation\n",
    "    x_tst, x_val, d_tst, d_val = train_test_split(x_tst, d_tst, test_size=0.4, random_state=41)\n",
    "    return x_trn, d_trn, x_val, d_val, x_tst, d_tst\n",
    "\n",
    "def bcf():\n",
    "    \"Load and prepare the Bioconcentration dataset.\"\n",
    "    bcf_trn = pd.read_csv(\"Grisoni_trn.csv\", delimiter='\\t')\n",
    "    bcf_tst = pd.read_csv(\"Grisoni_tst.csv\", delimiter='\\t')\n",
    "\n",
    "    x_trn = bcf_trn.iloc[:, 3:12].values\n",
    "    x_tst = bcf_tst.iloc[:, 3:12].values\n",
    "\n",
    "    # One-hot coding\n",
    "    d_trn = bcf_trn.iloc[:, 12].values.astype(int) - 1\n",
    "    d_trn = np.eye(3, dtype=int)[d_trn]\n",
    "    d_tst = bcf_tst.iloc[:,12].values.astype(int) - 1\n",
    "    d_tst = np.eye(3, dtype=int)[d_tst]\n",
    "\n",
    "    return x_trn, d_trn, x_tst, d_tst\n",
    "\n",
    "def standard(x):\n",
    "    \"Mean and stddev across samples\"\n",
    "    return np.mean(x, axis=0), np.std(x, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CellName: PlotData (#3)\n",
    "**CellType: Information**  \n",
    "**Cell instruction: Plotting the data**\n",
    "\n",
    "Here we just generate 100 cases for syn2 and the spiral dataset and plot them. \n",
    "\n",
    "Run the cell by entering into the cell and press \"CTRL Enter\". \n",
    "\n",
    "**Note!** This cell is not needed for the actual exercises, it is just to visualize the four different 2D synthetic classification data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = 0 means random, seed > 0 means fixed\n",
    "seed = 0\n",
    "np.random.seed(seed) if seed else None\n",
    "\n",
    "x,d = syn2(100)\n",
    "plt.figure()\n",
    "plt.scatter(x[:,0], x[:,1], c=d)\n",
    "\n",
    "x,d = twospirals(500, 3, 0)\n",
    "plt.figure()\n",
    "plt.scatter(x[:,0], x[:,1], c=d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CellName: Confusion (#4)\n",
    "**CellType: Needed**  \n",
    "**Cell Instruction: Plot the confusion matrix**\n",
    "\n",
    "This cell defines the functions need to plot a confusion matrix. A confusion matrix is a summary of the predictions made by a classifier. Each column of the matrix represents the instances of the predicted class while each row represents the instances of the actual class. The function 'plot_confusion_matrix' does the actual plotting, while the 'make_cm_plot' is the one that should be called by the user. See example of usage in the exercises. \n",
    "\n",
    "Run the cell by entering into the cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, target_names, title='Confusion matrix',\n",
    "                          cmap=None, normalize=True):\n",
    "    \"Plot a confusion matrix\"\n",
    "\n",
    "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "    misclass = 1 - accuracy\n",
    "\n",
    "    if cmap is None:\n",
    "        cmap = plt.get_cmap('Blues')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if target_names is not None:\n",
    "        tick_marks = np.arange(len(target_names))\n",
    "        plt.xticks(tick_marks, target_names, rotation=45)\n",
    "        plt.yticks(tick_marks, target_names)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        if normalize:\n",
    "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        else:\n",
    "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label\\n'\n",
    "               f'accuracy={accuracy:0.4f}; misclass={misclass:0.4f}')\n",
    "    plt.show()\n",
    "\n",
    "def make_cm_plot(model, inp, trg, label='Test data'):\n",
    "    \"\"\"\n",
    "    Compute and plot the confusion matrix\n",
    "    \"\"\"\n",
    "    print(f'*** Result for {label} ***')\n",
    "\n",
    "    num_classes = trg.shape[1]\n",
    "    y = model.predict(inp)\n",
    "\n",
    "    print(f'log_loss:   {log_loss(trg, y):.4f}')\n",
    "    d_class = trg.argmax(axis=1)\n",
    "    y_class = y.argmax(axis=1)\n",
    "    acc = (y_class==d_class).mean()\n",
    "    print(f'accuracy:   {acc:.4f}\\n')\n",
    "\n",
    "    class_names = [f'class {i+1}' for i in range(num_classes)]\n",
    "    print(classification_report(d_class, y_class, target_names=class_names))\n",
    "\n",
    "    confuTst = confusion_matrix(d_class, y_class)\n",
    "    plot_confusion_matrix(cm           = confuTst,\n",
    "                          normalize    = False,\n",
    "                          target_names = class_names,\n",
    "                          title        = f\"Confusion Matrix, {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CellName: MLP (#5)\n",
    "**CellType: Needed**  \n",
    "**Cell instruction: Defining the MLP model**\n",
    "\n",
    "This cell defines the MLP model. Several MLP hyperparameters are needed to define a model.\n",
    "Here is a list of them: **Note:** They can all be specified when you call\n",
    "this function in later cells. The ones specified in this cell are the default values.\n",
    "\n",
    "* inputs: the input dimension (integer)\n",
    "\n",
    "* output: the input dimension (integer)\n",
    "\n",
    "* layers: size of the network, eg `[5]` for a one hidden layer with 5 nodes and `[5, 3]` for a two layer network with 5 and 3 hidden nodes each.\n",
    "\n",
    "* activation: the activation function. Most common are\n",
    "    * `None` (linear)\n",
    "    * `nn.ReLU`\n",
    "    * `nn.Tanh`\n",
    "    * `nn.Sigmoid`\n",
    "        \n",
    "* output_activation: the activation function for the output nodes. Most common are\n",
    "    * `None` (linear)\n",
    "    * `nn.Sigmoid`\n",
    "    * `nn.Softmax`\n",
    "\n",
    "* dropout: Dropout parameter for each hidden layer. You can specipty a single number that will be used for all hidden layers. If you want different dropout parameters for each hidden layer, specify them as a list. Example, for a two hidden layer network `dropout = [0.5, 0.75]` means drop hidden nodes with probability 0.5 and 0.75 for the first and the second hidden layer, respectively. Note that we do not use dropout on the input nodes! Also, a value of 0 means no dropout (i.e. zero probability of removing a node).\n",
    "\n",
    "* l2regularization: L2 regularization strength. Like with the dropout, you can specify a single value or a list of values for the hidden layers.\n",
    "\n",
    "Run the cell by entering into the cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    \"A simple MLP with one or more fully connected layers\"\n",
    "    def __init__(self, *, inputs=1, outputs=1, nodes=[4], activation=nn.Tanh,\n",
    "                 out_activation=None, dropout=None, l2regularization=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs (int, optional): The number of input nodes.\n",
    "            outputs (int, optional): The number of output nodes.\n",
    "            nodes (list, optional): A list of layer sizes.\n",
    "            activation (list or class): Activation function (or None for linear).\n",
    "                Defaults to nn.Tanh. Can be a list of length len(nodes).\n",
    "            out_activation (optional): Activation function for output layer.\n",
    "            dropout (float or array-like, optional): Dropout fraction for hidden layers.\n",
    "            l2regularization (float or array-like, optional): Regularization strength for hidden layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Accept a value or array-like of length len(nodes)\n",
    "        if dropout is not None:\n",
    "            dropout = np.broadcast_to(dropout, len(nodes))\n",
    "        if activation is not None:\n",
    "            activation = np.broadcast_to(activation, len(nodes))\n",
    "        if l2regularization is not None:\n",
    "            l2regularization = np.broadcast_to(l2regularization, len(nodes))\n",
    "            self.l2regularization = []\n",
    "        else:\n",
    "            self.l2regularization = None\n",
    "\n",
    "        seqstack = OrderedDict()\n",
    "        prevn = inputs\n",
    "        for i, n in enumerate(nodes):\n",
    "            layer = nn.Linear(prevn, n, dtype=dtype_torch)\n",
    "            seqstack[f\"layer{i+1}\"] = layer\n",
    "            prevn = n\n",
    "            if activation is not None:\n",
    "                seqstack[f\"act{i+1}\"] = activation[i]()\n",
    "            if dropout is not None and dropout[i]:\n",
    "                seqstack[f\"drop{i+1}\"] = nn.Dropout(dropout[i])\n",
    "            if l2regularization is not None and l2regularization[i]:\n",
    "                self.l2regularization.append((l2regularization[i], layer))\n",
    "\n",
    "        seqstack[\"layerN\"] = nn.Linear(prevn, outputs, dtype=dtype_torch)\n",
    "        if out_activation is not None:\n",
    "            # Softmax needs the input dimension to be specified\n",
    "            if out_activation is nn.Softmax:\n",
    "                seqstack[\"actN\"] = out_activation(dim=1)\n",
    "            else:\n",
    "                seqstack[\"actN\"] = out_activation()\n",
    "        self.mlp_stack = nn.Sequential(seqstack)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"Apply the network stack on some input\"\n",
    "        return self.mlp_stack(x)\n",
    "\n",
    "    def regularization_loss(self):\n",
    "        \"Compute the total regularization cost\"\n",
    "        if self.l2regularization is None:\n",
    "            return 0\n",
    "        loss = 0\n",
    "        for lambd, layer in self.l2regularization:\n",
    "            loss = loss + lambd * torch.norm(layer.weight, p=2)\n",
    "        return loss\n",
    "\n",
    "    def predict(self, input_data):\n",
    "        \"\"\"\n",
    "        Apply the network on a set of input data.\n",
    "\n",
    "        Args:\n",
    "            input_data (np.ndarray or Tensor): Input data\n",
    "\n",
    "        Returns:\n",
    "            pred (np.ndarray or Tensor): Predicted output.\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        if isinstance(input_data, np.ndarray):\n",
    "            inp = torch.tensor(input_data, dtype=dtype_torch, device=device)\n",
    "            with torch.no_grad():\n",
    "                pred = self(inp)\n",
    "            return pred.cpu().detach().numpy()\n",
    "        with torch.no_grad():\n",
    "            return self(input_data.to(device))\n",
    "\n",
    "    def __str__(self):\n",
    "        s = super().__str__()\n",
    "        ps = [\"Named parameters:\"] + [\n",
    "            f\"{name}: {param.numel()}\" for name, param in\n",
    "             self.mlp_stack.named_parameters() if param.requires_grad]\n",
    "        totp = sum(p.numel() for p in self.mlp_stack.parameters() if p.requires_grad)\n",
    "        return s + f\"\\nTrainable parameters: {totp}\\n\" + \"\\n  \".join(ps) + \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CellName: Boundary (#6)\n",
    "**CellType: Needed**  \n",
    "**Cell Instruction: Decision boundary**\n",
    "\n",
    "This cell defines the function to plot the decision boundary for a 2D input binary MLP classifier. \n",
    "\n",
    "Run the cell by entering into the cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_boundary(X : np.ndarray, Y1 : np.ndarray, model):\n",
    "    \"\"\"\n",
    "        Plot classfication and decision boundary for binary classification problem\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): input\n",
    "            Y1 (np.ndarray): target\n",
    "            model (Network): the model\n",
    "\n",
    "        Returns:\n",
    "            None.\n",
    "    \"\"\"\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    # grid stepsize\n",
    "    h = 0.025\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    Z[Z > .5] = 1\n",
    "    Z[Z <= .5] = 0\n",
    "\n",
    "    Y_pr = model.predict(X).flatten()\n",
    "    Y = Y1.flatten()\n",
    "\n",
    "    Y_pr[Y_pr > .5] = 1\n",
    "    Y_pr[Y_pr <= .5] = 0\n",
    "    Y[(Y != Y_pr) & (Y == 0)] = 2\n",
    "    Y[(Y != Y_pr) & (Y == 1)] = 3\n",
    "\n",
    "    plt.figure()\n",
    "    #plt.contourf(xx, yy, Z, cmap=plt.cm.PRGn, alpha = .9)\n",
    "    plt.contour(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "    plt.scatter(X[Y == 1, 0], X[Y == 1, 1], marker='+', c='k')\n",
    "    plt.scatter(X[Y == 0, 0], X[Y == 0, 1], marker='o', c='k')\n",
    "\n",
    "    plt.scatter(X[Y == 3, 0], X[Y == 3, 1], marker = '+', c='r')\n",
    "    plt.scatter(X[Y == 2, 0], X[Y == 2, 1], marker = 'o', c='r')\n",
    "\n",
    "    plt.ylabel('x2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CellName: Statistics (#7)\n",
    "**CellType: Needed**  \n",
    "**Cell instruction: Present result for both classification and regression problems**\n",
    "\n",
    "This cell defines two functions that we are going to call using a trained model to calculate both error and performance measures. \n",
    "\n",
    "Run the cell by entering into the cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_classification(model : Network, dset : TensorDataset,\n",
    "                         *, label : str, loss_fn = None):\n",
    "    \"\"\"\n",
    "    Print classification statistics.\n",
    "\n",
    "    Args:\n",
    "        model (Network): The model.\n",
    "        dset (TensorDataset): Input and target data.\n",
    "        label (str): Training, test etc.\n",
    "        loss (optional): Loss function.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    pred = model.predict(dset.tensors[0])\n",
    "    targ = dset.tensors[1]\n",
    "    if loss_fn is not None:\n",
    "        loss = loss_fn(pred, targ)\n",
    "\n",
    "    if targ.shape[1] == 1:\n",
    "        # Binary\n",
    "        pred = pred >= .5\n",
    "        targ = targ >= .5\n",
    "        nof_p, tp, tn = [k.sum() for k in [targ, pred[targ], ~pred[~targ]]]\n",
    "        stats = {'Accuracy': (tp + tn) / len(targ),\n",
    "                 'Sensitivity': tp / nof_p,\n",
    "                 'Specificity': tn / (len(targ) - nof_p)}\n",
    "    else:\n",
    "        # One-hot\n",
    "        pred = pred.argmax(axis=1)\n",
    "        targ = targ.argmax(axis=1)\n",
    "        stats = {'Accuracy': (pred == targ).sum() / len(targ)}\n",
    "\n",
    "    if loss_fn is not None:\n",
    "        stats['Loss'] = loss\n",
    "\n",
    "    print(f\"*** STATISTICS for {label} Data ***\")\n",
    "    for l, v in stats.items():\n",
    "        print(f'{l:15} {v:.4f}')\n",
    "    print()\n",
    "\n",
    "def stats_regression(*, model : Network, dset : TensorDataset, label = 'Training'):\n",
    "    \"\"\"\n",
    "    Print regression statistics.\n",
    "        MSE = mean squeared error between target and predictions.\n",
    "        CorrCoeff = correlation coefficient for the scatter between predictions and target values.\n",
    "\n",
    "    Args:\n",
    "        model (Network): The model.\n",
    "        dset (TensorDataset): Input and target data.\n",
    "        label (str): Training, test etc.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    pred = model.predict(dset.tensors[0]).flatten()\n",
    "    targ = dset.tensors[1].flatten()\n",
    "\n",
    "    stats = {\n",
    "        'MSE' : ((targ - pred)**2).mean(),\n",
    "        'CorrCoeff': torch.corrcoef(torch.vstack((pred, targ)))[1,0] }\n",
    "\n",
    "    print('\\n', '#'*10, f'STATISTICS for {label} Data', '#'*10, '\\n')\n",
    "    for l, v in stats.items():\n",
    "        print(f'{l:15} {v:.4f}')\n",
    "    print('\\n', '#'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CellName: Training (#8)\n",
    "**CellType: Needed**  \n",
    "**Cell Instruction: Functions for training and testing the MLP model**\n",
    "\n",
    "This cell defines functions for training the model for a single epoch (`train_epoch`),\n",
    "evaluating the performance in the test data (`test`) and training and testing over\n",
    "many epochs (`train_loop`).\n",
    "\n",
    "The `train_loop` function takes a previously defined `MLPNetwork` model, two\n",
    "PyTorch `DataLoader`s that provide the data for training and test, and several\n",
    "hyperparameters:\n",
    "\n",
    "* loss_fn: The error function used during training. There are three common ones\n",
    "    * `nn.MSELoss` (mean squared error)\n",
    "    * `nn.BCELoss` (binary cross entropy)\n",
    "    * `nn.CrossEntropyLoss` (categorical cross entropy)\n",
    "\n",
    "* optimizer: The error minimization method, which is constructed with information about the model and a learning rate. Common choices are\n",
    "    * `torch.optim.SGD`\n",
    "    * `torch.optim.Adam`\n",
    "    * `torch.optim.Nadam`\n",
    "    * `torch.optim.RMSprop`\n",
    "\n",
    "* metrics: Additional metrics to compute and print besides the loss. We use the [torchmetrics package](https://lightning.ai/docs/torchmetrics/stable/) and pass the metric(s) as a dict with `{name: metric}`. Examples:\n",
    "    * `{'accuracy': torchmetrics.Accuracy(task='binary')}`\n",
    "    * `{'MSE': torchmetrics.MeanSquaredError()}`\n",
    "\n",
    "Run the cell by entering into the cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(*, model : Network, dataloader : DataLoader,\n",
    "                loss_fn, optimizer : torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Train a model for a single epoch.\n",
    "\n",
    "    Args:\n",
    "        model (Network): The network.\n",
    "        dataloader (DataLoader): Batch DataLoader with training data.\n",
    "        loss_fn (Loss): Loss function, e.g. nn.MSELoss.\n",
    "        optimizer (Optimizer): The optimizer used to update the network.\n",
    "\n",
    "    Returns:\n",
    "        train_loss (float): Training error over all batches.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)   # Move data to GPU if necessary\n",
    "        optimizer.zero_grad()   # Reset the gradients\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        train_loss += loss.item() * len(X)\n",
    "        loss = loss + model.regularization_loss()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return train_loss / len(dataloader.dataset)\n",
    "\n",
    "def test(*, model : Network, dataloader : DataLoader, loss_fn, metrics=[]):\n",
    "    \"\"\"\n",
    "    Test a model on a set of data.\n",
    "\n",
    "    Args:\n",
    "        model (Network): The network.\n",
    "        dataloader (DataLoader): DataLoader with data to test.\n",
    "        loss_fn (Loss): Loss function, e.g. nn.MSELoss.\n",
    "        metrics (iterable): Additional metrics from torchmetrics.\n",
    "\n",
    "    Returns:\n",
    "        loss (float): Mean error over all batches.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            loss += loss_fn(pred, y).item() * len(X)\n",
    "            for m in metrics:\n",
    "                m.update(pred, y)\n",
    "    return loss / len(dataloader.dataset)\n",
    "\n",
    "\n",
    "def train_loop(*, model : Network, train_dataloader : DataLoader,\n",
    "               val_dataloader : DataLoader = None, loss_fn,\n",
    "               optimizer : torch.optim.Optimizer, epochs : int,\n",
    "               print_every:int = 100, metrics=None, print_final=True):\n",
    "    \"\"\"\n",
    "    Train and optionally test a model.\n",
    "\n",
    "    Args:\n",
    "        model (Network): The network.\n",
    "        train_dataloader (DataLoader): Training data.\n",
    "        val_dataloader (DataLoader, optional): Validation data.\n",
    "        loss_fn (Loss): Loss function, e.g. nn.MSELoss.\n",
    "        optimizer (Optimizer): An optimizer from torch.optim.\n",
    "        epochs (int): Number of epochs to train for.\n",
    "        print_every (int, optional): Print loss every so many epochs. Defaults to 100.\n",
    "        metrics (dict(name: metric), optional): Record/print these additional metrics.\n",
    "        print_final(bool, optional): Print final metrics. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        train_losses (list(float)): Training loss during each epoch.\n",
    "        val_losses (list(float)): Validation loss after each epoch.\n",
    "        metrics_res (dict(name: list(float))): Values of metrics after each epoch.\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_loss = np.nan\n",
    "\n",
    "    # Move metrics to CPU/GPU and prepare for their output\n",
    "    metrics = {name: m.to(device) for name, m in (metrics or {}).items()}\n",
    "    metrics_res = {name: [] for name in metrics.keys()}\n",
    "\n",
    "    for t in range(epochs):\n",
    "        train_loss = train_epoch(model=model, dataloader=train_dataloader,\n",
    "                           loss_fn=loss_fn, optimizer=optimizer)\n",
    "        train_losses.append(train_loss)\n",
    "        if val_dataloader is not None:\n",
    "            for m in metrics.values():\n",
    "                m.reset()\n",
    "            val_loss = test(dataloader=val_dataloader, model=model,\n",
    "                            loss_fn=loss_fn, metrics=metrics.values())\n",
    "            val_losses.append(val_loss)\n",
    "            for name, m in metrics.items():\n",
    "                metrics_res[name].append(m.compute().cpu())\n",
    "        if (print_every > 0 and t % print_every == 0) or (\n",
    "                print_every >= 0 and t + 1 == epochs):\n",
    "            extras = [f\" {n} {v[-1]:<7f}\" if torch.isreal(v[-1])\n",
    "                      else f\" {n} {v[-1]}\"\n",
    "                      for n, v in metrics_res.items()]\n",
    "            print(f\"Epoch {t+1:<7d} train {train_loss:<7f} \"\n",
    "                  f\" validation {val_loss:<7f}\", \"\".join(extras))\n",
    "    if print_final:\n",
    "        print(\"\\n** Validation metrics after training **\\n\"\n",
    "              f\"Loss {val_losses[-1]:<7g}\")\n",
    "        for n, v in metrics_res.items():\n",
    "            if torch.isreal(v[-1]):\n",
    "                print(f\"{n} {v[-1]:<7g}\")\n",
    "            else:\n",
    "                print(f\"{n}:\")\n",
    "                print(v[-1])\n",
    "        print()\n",
    "    return train_losses, val_losses, metrics_res\n",
    "\n",
    "def plot_training(train_loss, val_loss, metrics_res={}):\n",
    "    \"Plot the training history\"\n",
    "    plt.figure()\n",
    "    plt.ylabel('Loss / Metric')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.plot(train_loss, label=\"Training loss\")\n",
    "    plt.plot(val_loss, label=\"Validation loss\")\n",
    "    for name, res in metrics_res.items():\n",
    "        if torch.isreal(res[0]):\n",
    "            plt.plot(res, label=name)\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "End of \"Needed\" and \"Information\" cells. Below are the cells for the actual exercise.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CellName: Ex1 (#9)\n",
    "**CellType: Exercise**  \n",
    "**Cell instruction: Instruction for questions 1-3**\n",
    "\n",
    "The cell below should be used for questions 1-3. For question 1 you can run the cell as it is (i.e. CTRL-Return). For the other questions you need to modify the cell in order to change hyperparameters etc. \n",
    "\n",
    "From now on we will talk about *performance*! It can be performance of a trained model on the training dataset or the performance on the validation dataset. What do we mean by performance? For classification problems we will provide 4 different measurements as printed by the *stats_classification* function. They are:\n",
    "* Sensitivity = fraction of correctly classified \"1\" cases\n",
    "* Specificity = fraction of correctly classified \"0\" cases\n",
    "* Accuracy = fraction of correctly classified cases\n",
    "* Loss = cross-entropy error (so low loss means good performance!)\n",
    "\n",
    "For the questions in this exercise, accuracy is an appropriate performance measure.\n",
    "\n",
    "## Question 1, single-node validation performance\n",
    "Here you are going to train a classifier for the *syn2* dataset. You are also going to use a validation dataset as an estimate of the *true* performance. Since we generate these datasets we can allow for a relatively large validation dataset in order to get a more accurate estimation of *true* performance. The default value in the cell is to generate 1000 validation data points. \n",
    "\n",
    "Now, use *syn2* (100 training data points) and train a *linear* MLP to separate the two classes, i.e. use a single hidden node. **What is the performance you get on the validation dataset?** \n",
    "\n",
    "**Note:** Use a fixed random seed for this exercise since you will compare with runs in the next questions.\n",
    "\n",
    "**Hint:** Remember from the first computer exercise that you should average over a few trained models.\n",
    "\n",
    "## Question 2, improving training performance\n",
    "You are now going to train this model to a high training accuracy! By increasing the number of hidden nodes we should be able to get better and better training performance. **(a) How many hidden nodes do you need to reach an accuracy >95% on your training dataset?** **(b) What is the performance on the validation data set?**\n",
    "\n",
    "**Hint:** Remember from the first computer exercise that overtraining often means finding a good local minimum of the loss function, which may require some tuning of the hyperparameters that control the training. This means that you may have to change the learning rate, batch size and the number of epochs. Since the *Adam* method is usually better than the vanilla *stochastic gradient descent*, it is used in the cells below as the default minimizer. \n",
    "\n",
    "## Question 3, optimizing validation performance\n",
    "However, we are almost always interested in optimal *validation* performance. You should now find the number of hidden nodes that optimize the validation performance. **(a) What is the optimal number of hidden nodes for the syn2 dataset in order to maximize your validation performance?** **(b) Try to give an explanation for the number you obtained.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# seed = 0 means random, seed > 0 means fixed\n",
    "seed = 3\n",
    "np.random.seed(seed) if seed else None\n",
    "\n",
    "# Generate training data\n",
    "x_trn, d_trn = syn2(100)\n",
    "x_val, d_val = syn2(1000)\n",
    "\n",
    "# Standardization of inputs\n",
    "mu, std = standard(x_trn)\n",
    "x_trn = (x_trn - mu) / std\n",
    "x_val = (x_val - mu) / std\n",
    "\n",
    "# Define the network, cost function and training settings\n",
    "model_ex1 = Network(\n",
    "    inputs = x_trn.shape[1],        # number of input nodes\n",
    "    outputs = 1,                    # if binary --> 1 |  regression--> num inputs | multi-class--> num of classes\n",
    "    nodes = [1],                    # number of nodes in hidden layer\n",
    "    activation = nn.Tanh,           # activation function in hidden layer\n",
    "    out_activation = nn.Sigmoid,    # activation function in output layer (if not linear)\n",
    "    ).to(device)                    # move data to GPU or keep with CPU\n",
    "\n",
    "# Optimization parameters\n",
    "opt_method = torch.optim.Adam   # minimization method\n",
    "learning_rate = 0.05            # learning rate\n",
    "loss_fn = nn.BCELoss()          # loss function, binary cross entropy\n",
    "number_epochs = 500\n",
    "minibatch_size = 25\n",
    "\n",
    "# Additional metrics to print {name: metric}\n",
    "metrics = {'accuracy': torchmetrics.Accuracy(\"binary\")}\n",
    "\n",
    "# Print a summary of the model\n",
    "print(model_ex1)\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = opt_method(model_ex1.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create datasets and batch loaders for the training and test data on the GPU or CPU\n",
    "dset_trn = TensorDataset(torch.tensor(x_trn, device=device, dtype=dtype_torch),\n",
    "                         torch.tensor(d_trn, device=device, dtype=dtype_torch))\n",
    "dl_trn = DataLoader(dset_trn, batch_size=minibatch_size)\n",
    "\n",
    "dset_val = TensorDataset(torch.tensor(x_val, device=device, dtype=dtype_torch),\n",
    "                         torch.tensor(d_val, device=device, dtype=dtype_torch))\n",
    "dl_val = DataLoader(dset_val, batch_size=minibatch_size)\n",
    "\n",
    "# Train the network and print the progress\n",
    "train_loss, val_loss, metrics_res = train_loop(\n",
    "    model=model_ex1,\n",
    "    train_dataloader=dl_trn,\n",
    "    val_dataloader=dl_val,\n",
    "    loss_fn=loss_fn,\n",
    "    metrics=metrics,\n",
    "    optimizer=optimizer,\n",
    "    print_every=100,\n",
    "    epochs=number_epochs)\n",
    "\n",
    "# Plot the training history\n",
    "plot_training(train_loss, val_loss, metrics_res)\n",
    "\n",
    "# Call the stats function to print out statistics for classification problems\n",
    "stats_classification(model_ex1, dset_trn, loss_fn=loss_fn, label=\"Training\")\n",
    "stats_classification(model_ex1, dset_val, loss_fn=loss_fn, label=\"Validation\")\n",
    "\n",
    "# Plot the decision boundary with respect to the training data\n",
    "decision_boundary(x_trn, d_trn, model_ex1)\n",
    "\n",
    "# Uncomment this to plot the decision boundary with respect to the validation data\n",
    "#decision_boundary(x_val, d_val, model_ex1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CellName: Ex2 (#10)\n",
    "**CellType: Exercise**  \n",
    "**Cell instruction: Instruction for questions 4-6**\n",
    "\n",
    "Now we are going to look at a regression problem. The data as described above (*regr2*) consists of 6 inputs (features) and one output (target) value. As in the previous exercise, a new data set is generated each time you call the *regr2* function. To get exactly the same data set between different calls, use a fixed seed. For this problem we can control the amount of noise added to the target value. We are going to use a relatively small training dataset (250) and a larger validation dataset (1000) to get a more robust estimation of the generalization performance, and 0.4 units of noise. For regression problems we also need new performance measures. The *stats_reg* function will give you two such measures:\n",
    "* MSE = mean squared error (low error means good performance)\n",
    "* CorrCoeff = Pearson correlation coefficient for the scatter plot between predicted and true values.\n",
    "\n",
    "The cell below can be used as a template for all questions regarding this regression problem.\n",
    "\n",
    "## Question 4, optimizing regression performance\n",
    "*Model selection based on the number of hidden nodes (in a single hidden layer).* Find the number of hidden nodes that gives best validation performance. **How many hidden nodes gives the best validation performance?** **What is the best validation MSE (or correlation coefficient) you get?**\n",
    "\n",
    "**Hint:** A good strategy is to start with a \"small\" model and increase the number of hidden nodes and monitor the validation result.\n",
    "\n",
    "## Question 5, improving generalization with L2 regularization\n",
    "*Model selection based on L2 (weight decay).* Instead of using the number of hidden nodes to control the complexity we can use a regularization term added to the error function. You are going to control the complexity by adding a *L2* regularizer (see the \"INPUT\" dictionary in the cell). For the L2 regularization to make sense we need a start model that is capable being overtrained. The suggestion is to use at least twice as many hidden nodes for this question compared to what you found in Q4. You should modify the *L2* value until you find the optimal validation performance. **(a) Present your optimal model (L2 value and number of hidden nodes) and the validation performance.** **(b) Do you obtain a better result compared to Q4?**\n",
    "\n",
    "**Hint:** When you test different values for a hyperparameter, it usually makes more sense to multiply with a constant factor than to add a constant term. For example, if you test five values in the range from 0.1 to 10, the values {0.1, 0.3, 1, 3, 10} are usually a better choice than {0.1, 2.5, 5, 7.5, 10}.\n",
    "\n",
    "## Question 6, improving generalization with dropout\n",
    "*Model selection based on dropout.* Instead of using the *L2* regularizer we can use dropout. In short, repeat Q5, but use the *dropout* parameter instead. **(a) Present your optimal model (dropout value and number of hidden nodes) and the validation performance.** **(b) Do you obtain a better result compared to Q4/Q5?** \n",
    "\n",
    "**Hint:** Using dropout may require even more hidden nodes to start with! \n",
    "\n",
    "## Extra question\n",
    "The extra questions is provided if you have extra time. **These question are not required for the course and do not influence any grading.** \n",
    "\n",
    "Repeat Q4-Q6 using two hidden layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# seed = 0 means random, seed > 0 means fixed\n",
    "seed = 11\n",
    "np.random.seed(seed) if seed else None\n",
    "\n",
    "# Generate training and validation data\n",
    "x_trn, d_trn = regr2(250, 0.4)\n",
    "x_val, d_val = regr2(1000, 0.4)\n",
    "\n",
    "# Standardization of both inputs and targets\n",
    "mu, std = standard(x_trn)\n",
    "x_trn = (x_trn - mu) / std\n",
    "x_val = (x_val - mu) / std\n",
    "\n",
    "mu, std = standard(d_trn)\n",
    "d_trn = (d_trn - mu) / std\n",
    "d_val = (d_val - mu) / std\n",
    "\n",
    "# Define the network, cost function and training settings\n",
    "model_ex2 = Network(\n",
    "    inputs = x_trn.shape[1],    # number of input nodes\n",
    "    outputs = 1,                # number of output nodes\n",
    "    nodes = [1],                # number of nodes in hidden layer\n",
    "    activation = nn.Tanh,       # activation function in hidden layer\n",
    "    out_activation = None,      # linear activation function in output layer\n",
    "    dropout = 0.0,                # dropout rate\n",
    "    l2regularization = 0.0       # L2 regularization lambda\n",
    "    ).to(device)                # move data to GPU or keep with CPU\n",
    "\n",
    "# Optimization parameters\n",
    "opt_method = torch.optim.Adam   # minimization method\n",
    "learning_rate = 0.01           # learning rate\n",
    "loss_fn = nn.MSELoss()          # loss function, MSE\n",
    "number_epochs = 500\n",
    "minibatch_size = 25\n",
    "\n",
    "# Additional metrics to print\n",
    "metrics = {}\n",
    "\n",
    "# Print a summary of the model\n",
    "print(model_ex2)\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = opt_method(model_ex2.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create datasets and batch loaders for the training and test data on the GPU or CPU\n",
    "dset_trn = TensorDataset(torch.tensor(x_trn, device=device, dtype=dtype_torch),\n",
    "                         torch.tensor(d_trn, device=device, dtype=dtype_torch))\n",
    "dl_trn = DataLoader(dset_trn, batch_size=minibatch_size)\n",
    "\n",
    "dset_val = TensorDataset(torch.tensor(x_val, device=device, dtype=dtype_torch),\n",
    "                         torch.tensor(d_val, device=device, dtype=dtype_torch))\n",
    "dl_val = DataLoader(dset_val, batch_size=minibatch_size)\n",
    "\n",
    "# Train the network and print the progress\n",
    "train_loss, val_loss, metrics_res = train_loop(\n",
    "    model=model_ex2,\n",
    "    train_dataloader=dl_trn,\n",
    "    val_dataloader=dl_val,\n",
    "    loss_fn=loss_fn,\n",
    "    metrics=metrics,\n",
    "    optimizer=optimizer,\n",
    "    print_every=100,\n",
    "    epochs=number_epochs)\n",
    "\n",
    "# Plot the training history\n",
    "plot_training(train_loss, val_loss, metrics_res)\n",
    "\n",
    "# Scatter plots of predicted and true values\n",
    "pred_trn = model_ex2.predict(x_trn)\n",
    "pred_val = model_ex2.predict(x_val)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(d_trn, pred_trn, 'g*', label='Predict vs True (Training)')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(d_val, pred_val, 'b*', label='Predict vs True (Validation)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Call the stat_reg to get MSE and correlation coefficient\n",
    "stats_regression(model=model_ex2, dset=dset_trn, label='Training')\n",
    "stats_regression(model=model_ex2, dset=dset_val, label='Validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CellName: Ex3 (#11)\n",
    "**CellType: Exercise**  \n",
    "**Cell instruction: Instruction for questions 7-8**\n",
    "\n",
    "For this exercise you are given a classification problem with fixed training, validation and test datasets. The data is the Japanse vowels dataset described in the first cell. Your task is to do model selection, coming up with your optimal MLP architecture together with the hyperparameters you used. We provide less code here: normalization of the input data and the definition of the MLP is missing. You need to provide that on your own.\n",
    "\n",
    "## Question 7, create MLP for binary classification\n",
    "**(a) Present an MLP with associated hyperparameters that maximizes the validation performance, and state the training, validation and test performance you obtained.**  \n",
    "**(b) Present your code.**\n",
    "\n",
    "**Hint 1:** \n",
    "Remember to normalize the input data.\n",
    "\n",
    "**Hint 2:** \n",
    "This problem is a 9-class classification problem, meaning that you should use a specific output activation function (*out_act_fun*) and a specific loss/error function (*cost_fun*).\n",
    "\n",
    "**Hint 3:**\n",
    "Place a line with three tildes (\\~\\~\\~) or backticks() above and below your code in the report:\n",
    "~~~\n",
    "    for a in ['Hello', 'World']:\n",
    "        print(a)\n",
    "~~~\n",
    "\n",
    "## Question 8, model selection criteria\n",
    "The typical goal is to have a high validation accuracy (i.e. the fraction of correctly classified cases). However, during training we typically monitor the *loss* of the validation data, to detect overtraining. Looking at both accuracy and loss, you may often find that the validation loss increases during training, but the validation accuracy stays constant. **Why can this happen?** **Given this situation, what would be your criteria to select the best model?** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load the data\n",
    "x_trn, d_trn, x_val, d_val, x_tst, d_tst = vowels()\n",
    "\n",
    "# YOUR CODE FOR NORMALIZATION\n",
    "\n",
    "num_classes = 9\n",
    "\n",
    "# YOUR CODE THAT DEFINES THE MLP\n",
    "\n",
    "# Define the model and optimization parameters\n",
    "model_ex3 = Network( ... )\n",
    "\n",
    "...\n",
    "\n",
    "# Additional metrics to print\n",
    "metrics = {}\n",
    "\n",
    "# Print a summary of the model\n",
    "print(model_ex3)\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = opt_method(model_ex3.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create datasets and batch loaders for the training and validation data on the GPU or CPU\n",
    "dset_trn = TensorDataset(torch.tensor(x_trn, device=device, dtype=dtype_torch),\n",
    "                         torch.tensor(d_trn, device=device, dtype=dtype_torch))\n",
    "dl_trn = DataLoader(dset_trn, batch_size=minibatch_size)\n",
    "\n",
    "dset_val = TensorDataset(torch.tensor(x_val, device=device, dtype=dtype_torch),\n",
    "                         torch.tensor(d_val, device=device, dtype=dtype_torch))\n",
    "dl_val = DataLoader(dset_val, batch_size=minibatch_size)\n",
    "\n",
    "\n",
    "# Train the network and print the progress\n",
    "train_loss, val_loss, metrics_res = train_loop(\n",
    "    model=model_ex3,\n",
    "    train_dataloader=dl_trn,\n",
    "    val_dataloader=dl_val,\n",
    "    loss_fn=loss_fn,\n",
    "    metrics=metrics,\n",
    "    optimizer=optimizer,\n",
    "    print_every=20,\n",
    "    epochs=number_epochs)\n",
    "\n",
    "# Plot the training history\n",
    "plot_training(train_loss, val_loss, metrics_res)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "make_cm_plot(model_ex3, x_trn, d_trn, 'Training data')\n",
    "make_cm_plot(model_ex3, x_val, d_val, 'Validation data')\n",
    "make_cm_plot(model_ex3, x_tst, d_tst, 'Test data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CellName: Ex4 (#12)\n",
    "**CellType: Exercise**  \n",
    "**Cell instruction: Instruction for question 9**\n",
    "\n",
    "For this exercise you are given a classification problem with a fixed training and test dataset. The data is the Bioconcentration dataset described in the first cell. Your task is to do model selection, coming up with your optimal MLP architecture together with the hyperparameters you used. We do not provide any python code for this question, only the small part that reads the data (next code cell).\n",
    "\n",
    "## Question 9, create MLP for multi-class problem\n",
    "**(a) Present an MLP with associated hyperparameters that maximizes the validation performance and give the test performance you obtained.** For this classification task there are not so many cases of class 2. In the training data there is: class 1: 345 cases, class 2: 48 cases, and class 3: 191 cases. One can end upp with situations that the network does not at all learn how to detect cases of class 2. **(b) What performance measure(s) do you think is(are) relevant when you select the optimal model for this problem?**  \n",
    "\n",
    "**Hint 1:** \n",
    "Remember to normalize input (but not output) data.\n",
    "\n",
    "**Hint 2:** \n",
    "Since there is no defined validation data set you need to split your original training data into training and validation data. You can use *sklearn.model_selection.train_test_split* or *sklearn.model_selection.KFold* to accomplish that, where the latter method does k-fold crossvalidation splits.\n",
    "\n",
    "**Hint 3:** \n",
    "This problem is a 3-class classification problem, meaning that you should use a specific output activation function (*out_activation*) and a specific loss/error function (*loss_fn*).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# seed = 0 means random, seed > 0 means fixed\n",
    "seed = 0\n",
    "np.random.seed(seed) if seed else None\n",
    "\n",
    "# Load Bioconcentration training and test data\n",
    "x_trn, d_trn, x_tst, d_tst = bcf()\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CellName: Ex5 (#13)\n",
    "**CellType: Exercise**  \n",
    "**Cell instruction: Instruction for question 10**\n",
    "\n",
    "For this exercise the task is to train a binary classifier for the spiral problem. The aim is to get *zero* classification error for the training data (there is no test or validation data) with a model that is *as small as possible* in terms of the number of trainable parameters. Also plot the boundary to see if it resembles a spriral. To pass this question you should at least try! The data is randomly generated and we suggest using at least 1000 data points to get \"good\" spirals.\n",
    "\n",
    "## Question 10, minimize network size\n",
    "**Train a classifier for the spiral problem with the aim of zero classification error with as small as possible model. Report the model you used.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# seed = 0 means random, seed > 0 means fixed\n",
    "seed = 0\n",
    "np.random.seed(seed) if seed else None\n",
    "    \n",
    "# Generate training data\n",
    "x_trn, d_trn = twospirals(1500, 3, 0)\n",
    "\n",
    "mu, std = standard(x_trn)\n",
    "x_trn = (x_trn - mu)/std\n",
    "\n",
    "# Define the network, cost function and training settings\n",
    "model_ex5 = Network(\n",
    "    inputs = x_trn.shape[1],        # number of input nodes\n",
    "    outputs = 1,                    # number of output nodes\n",
    "    nodes = [5],                    # number of nodes in hidden layer\n",
    "    activation = nn.Tanh,           # activation function in hidden layer\n",
    "    out_activation = nn.Sigmoid,    # activation function in output layer (if not linear)\n",
    "    dropout = 0,                    # dropout rate\n",
    "    l2regularization = 0            # L2 regularization lambda\n",
    "    ).to(device)                    # move data to GPU or keep with CPU\n",
    "\n",
    "# Optimization parameters\n",
    "opt_method = torch.optim.Adam   # minimization method\n",
    "learning_rate = 0.01            # learning rate\n",
    "loss_fn = nn.BCELoss()          # loss function, binary cross entropy\n",
    "number_epochs = 1500\n",
    "minibatch_size = 100\n",
    "\n",
    "# Additional metrics to print {name: metric}\n",
    "metrics = {'accuracy': torchmetrics.Accuracy(\"binary\")}\n",
    "\n",
    "# Print a summary of the model\n",
    "print(model_ex5)\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = opt_method(model_ex5.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create datasets and batch loaders for the training and test data on the GPU or CPU\n",
    "dset_trn = TensorDataset(torch.tensor(x_trn, device=device, dtype=dtype_torch),\n",
    "                         torch.tensor(d_trn, device=device, dtype=dtype_torch))\n",
    "dl_trn = DataLoader(dset_trn, batch_size=minibatch_size)\n",
    "\n",
    "# Train the network and print the progress\n",
    "train_loss, val_loss, metrics_res = train_loop(\n",
    "    model=model_ex5,\n",
    "    train_dataloader=dl_trn,\n",
    "    val_dataloader=dl_trn,\n",
    "    loss_fn=loss_fn,\n",
    "    metrics=metrics,\n",
    "    optimizer=optimizer,\n",
    "    epochs=number_epochs,\n",
    "    print_every=100)\n",
    "\n",
    "# Plot the training history\n",
    "plot_training(train_loss, val_loss, metrics_res)\n",
    "\n",
    "# Call the stats function to print out statistics for classification problems\n",
    "stats_classification(model_ex5, dset_trn, loss_fn=loss_fn, label=\"Training\")\n",
    "\n",
    "# Plot the decision boundary with respect to the validation data\n",
    "decision_boundary(x_trn, d_trn, model_ex5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# The report!\n",
    "We have added intructions inside this report template. As you write your report, remove the instructions.\n",
    "\n",
    "## Name\n",
    "\n",
    "## Introduction\n",
    "A few sentences about the overall theme of the exercise.\n",
    "\n",
    "## Answers to questions\n",
    "Provide enough information to clarify the meaning of your answers, so that they can be understood by someone who does not scroll up and read the entire instruction.\n",
    "\n",
    "The questions are repeated here, for clarity of what is demanded. If it does not fit your style to quote them verbatim, change the format.\n",
    "\n",
    "**Question 1**, single-node validation performance  \n",
    "What is the performance you get on the validation dataset?\n",
    "\n",
    "**Question 2**, improving training performance  \n",
    "**(a)** How many hidden nodes do you need to reach an accuracy >95% on your training dataset?  \n",
    "**(b)** What is the performance on the validation data set?\n",
    "\n",
    "**Question 3**, optimizing validation performance  \n",
    "**(a)** What is the optimal number of hidden nodes for the syn2 dataset in order to maximize your validation performance?  \n",
    "**(b)** Try to give an explanation for the number you obtained.\n",
    "\n",
    "**Question 4**, optimizing regression performance  \n",
    "**(a)** How many hidden nodes gives the best validation performance?  \n",
    "**(b)** What is the best validation MSE (or correlation coefficient) you get?\n",
    "\n",
    "**Question 5**, improving generalization with regularization  \n",
    "**(a)** Present your optimal model (L2 value and number of hidden nodes) and the validation performance.  \n",
    "**(b)** Do you obtain a better result compared to Q4?\n",
    "\n",
    "**Question 6**, improving generalization with dropout  \n",
    "**(a)** Present your optimal model (dropout value and number of hidden nodes) and the validation performance.  \n",
    "**(b)** Do you obtain a better result compared to Q4/Q5?\n",
    "\n",
    "**Question 7**, create MLP to solve the vowel problem  \n",
    "**(a)** Present an MLP with associated hyperparameters that maximizes the validation performance, and state the training, validation and test performance you obtained.  \n",
    "**(b)** Present your code.\n",
    "\n",
    "**Question 8**, model selection criteria  \n",
    "Why can the validation loss increase while the training loss stays constant?  \n",
    "Given this situation, what would be your criteria to select the best model?\n",
    "\n",
    "**Question 9**, create MLP for multi-class problem  \n",
    "**(a)** Present an MLP with associated hyperparameters that maximizes the validation performance and give the test performance you obtained.  \n",
    "**(b)** What performance measure(s) do you think is(are) relevant when you select the optimal model for this problem?\n",
    "\n",
    "**Question 10**, minimize network size  \n",
    "Train a classifier for the spiral problem with the aim of zero classification error with as small as possible model. Report the model you used.\n",
    "\n",
    "## Summary\n",
    "Connect the summary to your introduction, to provide a brief overview of your findings.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "530px",
    "width": "356.167px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
